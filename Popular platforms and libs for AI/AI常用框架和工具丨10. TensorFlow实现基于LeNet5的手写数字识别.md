<font color=red><b>代码实例，TensorFlow基于LeNet5实现手写数字识别，希望对您有所帮助。



## 环境说明
>**操作系统：Windows 10** 
> \
> **CUDA 版本为: 10.0**
> \
> **cudnn 版本为: 7.6.5**
> \
> **Python 版本为：Python 3.6.13**	
> \
> **tensorflow-gpu 版本为：1.13.1**
> \
> **注意CUDA、cudnn、Python、tensorflow版本之间的匹配**

## 一、模型训练

### 1.1 导入相关依赖

```python
# 导入相关依赖
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image, ImageOps
```

### 1.2 数据集准备

```python
data_folder = "./MNIST_data"  # 数据集路径
mnist = input_data.read_data_sets(data_folder, one_hot=True)  # 导入已经下载好的数据集
```

### 1.3 超参数设置
```python
# 参数
learning_rate = 0.003  # 学习率
num_steps = 1000  # 训练循环步数
batch_size = 128  # 批量大小
display_step = 100  # 每100步展示评价信息
```

### 1.4 网络相关模块初始化
```python
# 权重初始化
def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)


# 偏置初始化
def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)


# 卷积层初始化
def conv2d(x, W):
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')


# 池化层（最大池化）初始化
def max_pool_2x2(x):
    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
```

### 1.5 输入输出设置
```python
# 定义placeholder
X = tf.placeholder('float', shape=[None, 28 * 28])
Y = tf.placeholder('float', shape=[None, 10])
# 设置权重和偏置的向量格式
W = tf.Variable(tf.zeros(784, 10))
b = tf.Variable(tf.zeros(10))
# 改变输入x格式为4D向量
x_image = tf.reshape(X, [-1, 28, 28, 1])
```

### 1.6 搭建LeNet5网络
```python
# 第一层：卷积层+池化层
with tf.name_scope('layers1'):
    W_conv1 = weight_variable([5, 5, 1, 6])
    b_conv1 = bias_variable([6])
    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
    h_pool1 = max_pool_2x2(h_conv1)
    
# 第二层：卷积层+池化层
with tf.name_scope('layers2'):
    W_conv2 = weight_variable([5, 5, 6, 16])
    b_conv2 = bias_variable([16])
    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
    h_pool2 = max_pool_2x2(h_conv2)

# 两层全连接+一层输出层
with tf.name_scope('fc'):
    # 池化层的2D输出扁平化为1维
    h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 16])
    # 初始化第一个全连接层的权值
    W_fc1 = weight_variable([7 * 7 * 16, 120])  # 上一层有7*7*16个神经元，全连接层有120个神经元
    b_fc1 = bias_variable([120])  # 120 个节点
    # 第一个全连接层的输出
    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
    # 初始化第二个全连接层
    W_fc2 = weight_variable([120, 84])  # 上一层有120个神经元
    b_fc2 = bias_variable([84])  # 84 个节点
    # 第二个全连接层的输出
    h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)
    # 初始化输出层
    W_fc3 = weight_variable([84, 10])  # 上一层有84个神经元
    b_fc3 = bias_variable([10])  # 10个节点

# 输出层的输出
y_conv = tf.nn.softmax(tf.matmul(h_fc2, W_fc3) + b_fc3)
```

### 1.7 评估指标
```python
# 交叉熵代价函数
loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_conv, labels=Y))
# 使用AdamOptimizer进行优化
train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss_op)
# 结果存放在布尔型列表
correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(Y, 1))  # argmax返回张量中最大的值所在的位置
# 求准确率
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
```

### 1.8 进行训练并测试

```python
# 初始化
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# 进行训练
for step in range(1, num_steps - 4):
    batch_x, batch_y = mnist.train.next_batch(batch_size)
    # 运行优化器，反向传播
    sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})
    if step % display_step == 0 or step == 1:
        # 计算loss和acc
        loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y})
        print('Step ' + str(step) + ", Minibatch Loss = " + '{:.4f}'.format(
            loss) + ', Training Accuracy = ' + '{:.3f}'.format(acc))

print("Optimization Finished")
# 在MNIST test images上验证效果
print('Testing Accuracy:', sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))
```

## 二、本地识别

### 2.1 生成本地图片的标签
```python
# 生成本地图片标签
def make_label(label_num):
    label = np.zeros((1, 10), dtype='float32')
    label[:, label_num] = 1.0
    return label
label_test = make_label(5)
```
### 2.2 列表存储本地图片名称
```python
# 本地图片名称保存在imag_names的列表中
image_names = ['num0.png', 'num1.png', 'num2.png', 'num3.png', 'num4.png',
               'num5.png', 'num6.png', 'num7.png', 'num8.png', 'num9.png']
```

### 2.3 本地图片处理
```python
# 本地图片处理
img = Image.open('./num-draft/' + image_name)  # 导入本地图片
img_gray = np.array(ImageOps.grayscale(img))  # 图片灰度化
img_inv = (255 - img_gray) / 255.0  # 转成白底黑字，以适应MNIST数据集中的数据(黑底白字)。再进行归一化处理
image = img_inv.reshape((1, 28*28))  # 转四维数据，CNN神经网络预测需要四维数据
```
### 2.4 预测并打印结果
```python
# 预测
test_acc, test_value = sess.run([accuracy, y_conv], feed_dict={X: image, Y: label_test})  # 预测
# print(prediction)  # 打印预测结果的数组
print('预测的图片是: ', image_name, 'AI判断的数字是{}'.format(list(test_value[0]).index(test_value[0].max())))  # 打印预测结果
```

## 三、 完整代码
```python
# =====-*- coding: utf-8 -*-=====
# @Time  : 2022/3/18 19:29
# @Author: AXYZdong
# @File  : tf-dong.py
# @IDE   : Pycharm
# ===============================
# 导入相关依赖
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image, ImageOps

data_folder = "./MNIST_data"  # 数据集路径
mnist = input_data.read_data_sets(data_folder, one_hot=True)  # 导入已经下载好的数据集

# 参数
learning_rate = 0.003  # 学习率
num_steps = 1000  # 训练循环步数
batch_size = 128  # 批量大小
display_step = 100  # 每100步展示评价信息


# 权重初始化
def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)


# 偏置初始化
def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)


# 卷积层初始化
def conv2d(x, W):
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')


# 池化层（最大池化）初始化
def max_pool_2x2(x):
    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')


# 定义placeholder
X = tf.placeholder('float', shape=[None, 28 * 28])
Y = tf.placeholder('float', shape=[None, 10])
# 设置权重和偏置的向量格式
W = tf.Variable(tf.zeros(784, 10))
b = tf.Variable(tf.zeros(10))
# 改变输入x格式为4D向量
x_image = tf.reshape(X, [-1, 28, 28, 1])

# 第一层：卷积层+池化层
with tf.name_scope('layers1'):
    W_conv1 = weight_variable([5, 5, 1, 6])
    b_conv1 = bias_variable([6])
    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
    h_pool1 = max_pool_2x2(h_conv1)
    
# 第二层：卷积层+池化层
with tf.name_scope('layers2'):
    W_conv2 = weight_variable([5, 5, 6, 16])
    b_conv2 = bias_variable([16])
    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
    h_pool2 = max_pool_2x2(h_conv2)

# 两层全连接+一层输出层
with tf.name_scope('fc'):
    # 池化层的2D输出扁平化为1维
    h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 16])
    # 初始化第一个全连接层的权值
    W_fc1 = weight_variable([7 * 7 * 16, 120])  # 上一层有7*7*16个神经元，全连接层有120个神经元
    b_fc1 = bias_variable([120])  # 120 个节点
    # 第一个全连接层的输出
    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
    # 初始化第二个全连接层
    W_fc2 = weight_variable([120, 84])  # 上一层有120个神经元
    b_fc2 = bias_variable([84])  # 84 个节点
    # 第二个全连接层的输出
    h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)
    # 初始化输出层
    W_fc3 = weight_variable([84, 10])  # 上一层有84个神经元
    b_fc3 = bias_variable([10])  # 10个节点

# 输出层的输出
y_conv = tf.nn.softmax(tf.matmul(h_fc2, W_fc3) + b_fc3)

# 交叉熵代价函数
loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_conv, labels=Y))
# 使用AdamOptimizer进行优化
train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss_op)
# 结果存放在布尔型列表
correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(Y, 1))  # argmax返回张量中最大的值所在的位置
# 求准确率
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

# 初始化
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# 进行训练
for step in range(1, num_steps - 4):
    batch_x, batch_y = mnist.train.next_batch(batch_size)
    # 运行优化器，反向传播
    sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})
    if step % display_step == 0 or step == 1:
        # 计算loss和acc
        loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y})
        print('Step ' + str(step) + ", Minibatch Loss = " + '{:.4f}'.format(
            loss) + ', Training Accuracy = ' + '{:.3f}'.format(acc))

print("Optimization Finished")
# 在MNIST test images上验证效果
print('Testing Accuracy:', sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))


# 生成本地图片标签
def make_label(label_num):
    label = np.zeros((1, 10), dtype='float32')
    label[:, label_num] = 1.0
    return label


label_test = make_label(5)

# 本地图片名称保存在imag_names的列表中
image_names = ['num0.png', 'num1.png', 'num2.png', 'num3.png', 'num4.png',
               'num5.png', 'num6.png', 'num7.png', 'num8.png', 'num9.png']

# 批量预测本地图片
for image_name in image_names:
    # 本地图片处理
    img = Image.open('./num-draft/' + image_name)  # 导入本地图片
    img_gray = np.array(ImageOps.grayscale(img))  # 图片灰度化
    img_inv = (255 - img_gray) / 255.0  # 转成白底黑字，以适应MNIST数据集中的数据(黑底白字)。再进行归一化处理
    image = img_inv.reshape((1, 28*28))  # 转四维数据，CNN神经网络预测需要四维数据
    # 预测
    test_acc, test_value = sess.run([accuracy, y_conv], feed_dict={X: image, Y: label_test})  # 预测
    # print(prediction)  # 打印预测结果的数组
    print('预测的图片是: ', image_name, 'AI判断的数字是{}'.format(list(test_value[0]).index(test_value[0].max())))  # 打印预测结果
    # 准备显示
    plt.subplot(2, 5, image_names.index(image_name) + 1)
    plt.imshow(Image.open('./num-draft/' + image_name))
    plt.yticks([])
    plt.title(f'predict:{np.argmax(test_value)}', color='blue')

# 展示图像
plt.text(0, 40, 'By AXYZdong')  # 水印
plt.show()
```
### 3.1 运行结果
Step输出信息：

```python
Step 1, Minibatch Loss = 2.2934, Training Accuracy = 0.211
Step 100, Minibatch Loss = 1.6348, Training Accuracy = 0.828
Step 200, Minibatch Loss = 1.6091, Training Accuracy = 0.852
Step 300, Minibatch Loss = 1.5082, Training Accuracy = 0.953
Step 400, Minibatch Loss = 1.5660, Training Accuracy = 0.906
Step 500, Minibatch Loss = 1.5606, Training Accuracy = 0.898
Step 600, Minibatch Loss = 1.4837, Training Accuracy = 0.977
Step 700, Minibatch Loss = 1.4859, Training Accuracy = 0.977
Step 800, Minibatch Loss = 1.4948, Training Accuracy = 0.969
Step 900, Minibatch Loss = 1.5020, Training Accuracy = 0.961
Optimization Finished
```
测试准确率：**97.45%**

![在这里插入图片描述](https://img-blog.csdnimg.cn/6a26d5264e5642b596eb3c53b6506be0.png#pic_center)
<p align="center">▲ 窗口输出的打印信息</p>

![在这里插入图片描述](https://img-blog.csdnimg.cn/03691255d6cd4f56873a7ecdbcdf93db.png#pic_center)
<p align="center">▲ 批量识别图像展示</p>


<center><strong>—— END ——</strong></center>

