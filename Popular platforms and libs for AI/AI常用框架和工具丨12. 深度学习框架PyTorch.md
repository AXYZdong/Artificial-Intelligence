<font color=red><b>深度学习框架PyTorch，AI常用框架和工具之一。理论知识结合代码实例，希望对您有所帮助。

![在这里插入图片描述](https://img-blog.csdnimg.cn/583901b024ea4849ac3acd3840176c23.png#pic_center)

## 环境说明
>**操作系统：Windows 10** 
> \
> **CUDA 版本为: 10.0**
> \
> **cudnn 版本为: 7.6.5**
> \
> **Python 版本为：Python 3.6.13**	
> \
> **PyTorch 版本为：1.0.0**
> \
> **注意CUDA、cudnn、Python、PyTorch版本之间的匹配**

## PyTorch安装
按照上述环境说明，conda安装

- Windows和Linux可以使用以下命令

```python
conda install pytorch==1.0.0 torchvision==0.2.1 cuda100 -c pytorch
```
- OSX可以使用以下命令

```python
 conda install pytorch==1.0.0 torchvision==0.2.1 -c pytorch
```

其他**版本匹配问题**以及**使用pip安装**可以参考官方网址：

[https://pytorch.org/get-started/previous-versions/](https://pytorch.org/get-started/previous-versions/)

## 一、PyTorch简介
### 1.1 Torch
Torch 是一个科学计算框架，广泛支持将 GPU 放在首位的机器学习算法。由于简单快速的脚本语言、LuaJIT 和底层 C/CUDA 实现，它易于使用且高效。

Torch 的核心是流行的神经网络和优化库，它们易于使用，同时在实现复杂的神经网络拓扑时具有最大的灵活性。您可以构建任意神经网络图，并以高效的方式在 CPU 和 GPU 上并行化它们。

核心功能总结：
- 强大的 N 维数组
- 许多用于索引、切片、转置的例程……
- 惊人的 C 接口，通过 LuaJIT
- 线性代数例程
- 神经网络和基于能量的模型
- 数值优化例程
- 快速高效的 GPU 支持
- 可嵌入的，带有 iOS 和 Android 后端的端口

### 1.2 从Torch到PyTorch 
| Lua | Torch | Python | PyTorch  |
|--|--|--|--|
| Lua是一种轻量小巧的脚本语言，用标准C语言编写并以源代码形式开放，其设计目的是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能。 | Torch是采用C语言作为底层，然后lua语言为接口的深度学习库。 |简单而灵活的编程语言，在人工智能领域有着广泛的应用，相对于lua语言而言流行度更广。|Python+Torch $\to$ PyTorch 。

### 1.3 PyTorch 
PyTorch是一个开源的Python机器学习库，基于Torch，用于自然语言处理等应用程序。

2017年1月，由Facebook人工智能研究院（FAIR）基于Torch推出了PyTorch。它是一个基于Python的可续计算包。

提供**两个高级功能**：

1、具有强大的GPU加速的张量计算（如NumPy）。

2、包含自动求导系统的深度神经网络。

**特点**：

- 用户友好：PyTorch提供易于使用的API，它在Python上运行，操作非常简单。这个框架中的代码执行非常简单。

- 简单：PyTorch库认为是Pythonic，它可以利用Python环境提供的所有服务和功能。

- 自由化：PyTorch提供了一个提供动态计算图的出色平台。因此，用户可以在运行时更改它们。


## 二、PyTorch中张量操作
### 2.1 torch
| 方法 | 说明 |
|--|--|
torch.is_tensor(obj)|判断是否是张量。
torch.is_storage|如果obj是一个pytorchstorage对象，则返回True。
torch.eye|创建一个数据为对角矩阵的张量。
torch.from_numpy|将numpy.ndarray转换为pytorch的Tensor。
torch.ones|返回一个全为1 的张量。
torch.rand|返回一个张量，包含了从区间[0,1)的均匀分布中抽取的一组随机数。

### 2.2 张量的常见操作
| 方法 | 说明 |
|--|--|
torch.index_select|索引
torch.nonzero|获取非零元素的索引
torch.unbind<br>Tensor[n,m]|切片
torch.cat|连接
torch.chunk|分块
torch.gather|聚合
torch.split|切分


### 2.3 张量的其他操作
| 方法 | 说明 |
|--|--|
torch.t (input, out=None)|输入一个矩阵（2维张量），并转置。
torch.transpose (input, dim0, dim1, out=None)|返回输入矩阵input的转置。交换维度dim0和dim1
torch.eq (input, other, out=None)|比较元素相等性
torch.equal (tensor1, tensor2)|如果两个张量有相同的形状和元素值，则返回True ，否则False。
torch.sort (input, dim=None, descending=False, out=None)|按照指定维度排序
torch.topk (input, k, dim=None, largest=True, sorted=True, out=None)|给定dim维度返回输入张量input中k 个最大值。

## 三、PyTorch模块介绍和使用
### 3.1 torch.nn
`torch.nn.Module`：pytorch中所有网络的基类，在创建模型时要继承这个类。

- 添加层：`add_module(name, module)`
- 获取子模型：`children()`
- 设备切换：`cuda/cpu(device_id=None)`

### 3.2 Containers
- torch.nn.Module
- torch.nn.ModuleList
- torch.nn.Sequential
- torch.nn.ParameterList

### 3.3 Layers
- Liner layers：`torch.nn.Linear(in_features, out_features, bias=True)`
- Dropout layers：`torch.nn.Dropout(p=0.5, inplace=False)`
- 激活函数：
                 1. `torch.nn.ReLU(inplace=False)`
                 2. `torch.nn.Tanh`
                 3. `torch.nn.Softmax`
### 3.4 CNN
1. 卷积层：`torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)`
2. 池化层：`torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)`
3. 标准化：`torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True)`
### 3.5 Recurrent layers
| layer | API |
|--|--|
RNN|torch.nn.RNN( args, * kwargs)
LSTM|torch.nn.LSTM( args, * kwargs)
GRU|torch.nn.GRU( args, * kwargs)

### 3.6 Loss functions
- `torch.nn.MSELoss(size_average=True)`：均方误差损失函数，常用于**回归预测**。
- `torch.nn.CrossEntropyLoss(weight=None, size_average=True)`：交叉熵损失函数，常用于**多分类**。
### 3.7 Optimizer
| API| Description |
|--|--|
torch.optim.SGD|随机梯度下降算法
torch.optim.RMSprop|RMSProp算法
torch.optim.Adagrad|AdaGrad算法
torch.optim.Adam|Adam算法

## 四、代码实例
### 4.1 创建张量

```python
import torch
# 创建特殊张量
a = torch.empty(3, 2)
b = torch.zeros(3, 2)
c = torch.ones(3, 2)
# 通过数据创建张量
d = torch.tensor([[1, 2], [3, 4]])
# 创建随机数据的张量
e = torch.rand(3, 2)

print('未初始化的张量:', a)
print('全0张量:', b)
print('全1张量:', c)
print('通过数据创建的张量:', d)
print('生成的随机张量', e)
```

运行结果

```python
未初始化的张量: tensor([[0., 0.],
        [0., 0.],
        [0., 0.]])
全0张量: tensor([[0., 0.],
        [0., 0.],
        [0., 0.]])
全1张量: tensor([[1., 1.],
        [1., 1.],
        [1., 1.]])
通过数据创建的张量: tensor([[1, 2],
        [3, 4]])
生成的随机张量 tensor([[0.0993, 0.6424],
        [0.3217, 0.2181],
        [0.7177, 0.4080]])
```

### 4.2 张量操作

```python
# =====-*- coding: utf-8 -*-=====
# @Time  : 2022/3/23 19:13
# @Author: AXYZdong
# @File  : dong-PyTorch.py
# @IDE   : Pycharm
# ===============================
import torch

x = torch.rand([2, 3])
y = torch.zeros([3, 3])
z = torch.ones([2, 3])

a = torch.is_tensor(x)  # 判断是否为张量
b = x[0, 2]  # 切片
c = x.size()  # 维度
d = torch.cat((x, y), 0)  # 合并
e = x + z  # 张量相加，或者使用 torch.add(x,y)
f = torch.t(x)  # 转置
g = torch.eq(x, z)  # 比较元素相等性
h = torch.equal(x, z)  # 两个张量有相同的形状和元素值，则返回True ，否则False

print('x是否为张量:', a)
print('对x进行切片结果是:', b)
print('x张量维度是:', c)
print('x,y张量合并后为:', d)
print('x+z结果为:', e)
print('x张量转置后为:', f)
print('x,z元素是否相等:', g)
print('x,z是否有相同的形状和元素值:', h)
```

运行结果
```python
x是否为张量: True
对x进行切片结果是: tensor(0.2509)
x张量维度是: torch.Size([2, 3])
x,y张量合并后为: tensor([[0.9525, 0.7483, 0.2509],
        [0.8677, 0.7387, 0.2237],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000]])
x+z结果为: tensor([[1.9525, 1.7483, 1.2509],
        [1.8677, 1.7387, 1.2237]])
x张量转置后为: tensor([[0.9525, 0.8677],
        [0.7483, 0.7387],
        [0.2509, 0.2237]])
x,z元素是否相等: tensor([[0, 0, 0],
        [0, 0, 0]], dtype=torch.uint8)
x,z是否有相同的形状和元素值: False
```


**参考文献**
- [1] https://education.huaweicloud.com/courses/course-v1:HuaweiX+CBUCNXE081+Self-paced/about
- [2] http://torch.ch/
- [3] https://baike.baidu.com/item/PyTorch/24269838?fr=aladdin


<center><strong>—— END ——</strong></center>

