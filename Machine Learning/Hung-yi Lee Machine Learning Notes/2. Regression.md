## 一、回归（Regression）
回归（Regression）：找到一个函数Function，通过输入一个特征 $x$，输出一个数值 $Scalar$ 。

**应用举例**
- 股市预测（Stock market forecast）

- 自动驾驶（Self-driving Car）

- 商品推荐（Recommendation）

- Pokemon精灵攻击力预测（Combat Power of a pokemon）：

## 二、模型步骤
### 2.1 模型假设 - 线性模型
- 一元线性模型（单个特征）

模型表示：$y=b+wx$

- 多元线性模型（多个特征）

模型表示：$y=b+\sum w_ix_i$
>- $x_i$：各种特征(fetrure) 
>- $w_i$：各个特征的权重
>- b：偏移量

### 2.2 模型评估 - 损失函数
单个特征：$x_{cp}$。

定义 $x^1$ 是进化前的CP值， $\hat{y}^1$ 为进化后的CP值，$\hat{}$ 所代表的的是真实值。

收集10组真实值，有了这些真实的数据，那我们怎么衡量模型的好坏呢？从数学的角度来讲，我们使用距离。求【进化后的CP值】与【模型预测的CP值】差，来判定模型的好坏。也就是使用损失函数（Loss function） 来衡量模型的好坏。

$$
L(f)=\sum_{n=1}^{10}(\hat{y}^n-f(x_{cp}^n))^2
\to L(w,b)=\sum_{n=1}^{10}(\hat{y}^n-(b+w\cdot x_{cp}^n)^2
$$

![在这里插入图片描述](https://img-blog.csdnimg.cn/266c273f984b4dda9d7eb1706bbc6986.png)

<p align="center"> ▲ 损失函数（Loss Function）</center>


将 $w$ 和 $b$ 在二维坐标中展示

- 图中每一个点代表着一个模型对应的 $w$ 和 $b$；
- 颜色越深代表模型更优。

![在这里插入图片描述](https://img-blog.csdnimg.cn/f3a7f8fcef994e9f846ab3964e1ba330.png)

<p align="center">▲ w 和 b 在二维坐标中展示 </center>


### 2.3 模型优化 - 梯度下降
单个特征：$x_{cp}$。

如何筛选出最优模型（即找出使得 Loss Function 最小的 $w$ 和 $b$）

![在这里插入图片描述](https://img-blog.csdnimg.cn/3ed816a4fcaa4f1cafd999562ddb7675.png)

<p align="center">▲ 定义f* </center>


- 先从最简单的只有一个参数 $w$ 入手，定义 $w^* = arg\ \underset{x}{\operatorname{\min}} L(w)$

步骤1：随机选取一个 $w^0$ 

步骤2：计算微分，也就是当前的斜率，根据斜率来判定移动的方向
- 大于0向右移动（增加ww）
- 小于0向左移动（减少ww）

步骤3：根据学习率移动

重复步骤2和步骤3，直到找到最低点

![在这里插入图片描述](https://img-blog.csdnimg.cn/d785d16228d94ec686e9661fc7737a6d.png)
<p align="center">▲ 梯度下降过程 </center>

- 对于两个参数 $w$ 和 $b$，过程与上述的一个参数类似，需要做的也是偏微分。

![在这里插入图片描述](https://img-blog.csdnimg.cn/60fae61ea6ea49f38197be3e3e548789.png)
<p align="center">▲ 两个参数偏微分过程 </center>
 
 **梯度下降算法在现实世界中面临的挑战**
- 问题1：当前最优（Stuck at local minima）
- 问题2：等于0（Stuck at saddle point）
- 问题3：趋近于0（Very slow at the plateau）

![在这里插入图片描述](https://img-blog.csdnimg.cn/9be70100560d42ecb436b301fd82f9b3.png)
<p align="center">▲ 梯度下降面临的问题 </center>

在线性模型里面都是一个碗的形状（山谷形状），梯度下降基本上都能找到最优点，但是再其他更复杂的模型里面，就会遇到 问题2 和 问题3 。

**验证模型好坏**

使用**训练集和测试集的平均误差**来验证模型的好坏。

## 三、过拟合（Overfitting）
在简单的模型基础上，可以进行优化，选择更复杂的模型（一元N次线性模型），或者说使用**多项式**来拟合。

如果我们选择较高次方的模型，在训练集上面表现更为优秀的模型，在测试集上效果可能反而变差了。这就是模型在训练集上过拟合的问题。

![在这里插入图片描述](https://img-blog.csdnimg.cn/f7b4b4f1da1a4d49ac8859932e5cc661.png)
<p align="center">▲ 过拟合（Overfitting）的问题 </center>

## 四、正则化（Regularization）
对于更多特征，但是权重 $w$ 可能会使某些特征权值过高，仍旧导致overfitting，可以加入正则化。

$$
损失函数:L=\sum_{n}(\hat{y}^n-(b+\sum w_ix_i)^2\\[2ex]
引入正则化项 \to L=\sum_{n}(\hat{y}^n-(b+\sum w_ix_i)^2+\lambda\sum (w_i)^2 \\[2ex]
\lambda\sum (w_i)^2：正则化项。
$$


![在这里插入图片描述](https://img-blog.csdnimg.cn/043875698afc40cc8ad6c4ff8291d967.png)
<p align="center">▲ 正则化（Regularization） </center>

![在这里插入图片描述](https://img-blog.csdnimg.cn/d50354fdbe054a348124ce911859346f.png)
<p align="center">▲调节 λ 获得最好的模型 </center>

## 五、总结
Datawhale组队学习，李宏毅《机器学习》Task2. Regression（回归），主要包括回归的定义、创建模型的步骤、如何优化模型、优化模型过程中可能出现的问题以及使用正则化来解决过拟合的问题。
