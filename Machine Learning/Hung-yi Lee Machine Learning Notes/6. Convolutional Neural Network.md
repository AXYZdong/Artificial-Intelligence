## 一、为什么CNN用于图像处理

当我们用fully connect feedforward network来做图像处理的时候，往往我们会需要太多的参数，举例来说，假设这是一张100 *100的彩色图(一张很小的imgage)，你把这个拉成一个vector，(它有多少个pixel)，它有100 *100 3的pixel。

如果是彩色图的话，每个pixel需要三个value来描述它，就是30000维(30000 dimension)，那input vector假如是30000dimension，那这个hidden layer假设是1000个neural，那么这个hidden layer的参数就是有30000 *1000，那这样就太多了。

那么CNN做的事就是简化neural network的架构。我们根据对图像的认识，某些weight用不上的，我们一开始就把它滤掉。不是用fully connect feedforward network，而是用比较少的参数来做图像处理这件事。所以CNN比一般的DNN还要简单。

![在这里插入图片描述](https://img-blog.csdnimg.cn/c885ff95381b44329d9e6e2f52e216ad.png)
<p align="center">▲ 为什么CNN用于图像处理 </center>

CNN就是用power-knowledge去把原来fully connect layer中一些参数拿掉。

为什么我们有可能把一些参数拿掉(为什么可以用比较少的参数可以来进行图像处理)

- 一些特征图像远小于整个图像
![在这里插入图片描述](https://img-blog.csdnimg.cn/dc02d20c55824dd08a4ba206dea628d4.png)
- 同样的特征会出现在不同的区域

![在这里插入图片描述](https://img-blog.csdnimg.cn/67897a49805a42c59d355228e25440dd.png)

- 像素缩放不会对图像造成太大影响

![在这里插入图片描述](https://img-blog.csdnimg.cn/6d42d7fbb24048459734529743314180.png)
 
## 二、CNN架构
首先input一张image，这张image会通过convolution layer，接下来是max pooling，然后再做convolution，再做max pooling。

上述过程可以反复无数次（反复多少次你是要事先决定的，它就是network的架构（就像neural有几层一样），要做几层的convolution，做几层的Max Pooling，在定neural架构的时候，要事先决定好）。

做完要做的convolution和Max Pooling以后，就要进行flatten，再把flatten的output丢到一般fully connected feedforward network，然后得到图像识别的结果。

![在这里插入图片描述](https://img-blog.csdnimg.cn/5a732df244594acb9bc3a27f5ff9e9a9.png)
<p align="center">▲ CNN架构 </center>

## 三、卷积（Convolution）

第一个filter是一个3* 3的matrix，把这个filter放在image的左上角，把filter的9个值和image的9个值做内积，两边都是1,1,1（斜对角），内积的结果就得到3。

（移动多少是事先决定的）移动的距离叫做stride（stride的值也可以事先设置），内积等于-1。图中的stride等于1。

![在这里插入图片描述](https://img-blog.csdnimg.cn/9973510fcb4c4838bfd7f477f5d6edc6.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/31e71a22a76b41ca950493ef39c57d4f.png)

<p align="center">▲ 如何Convolution </center>


## 四、Convolution和Fully Connected之间的关系
将stride=1（移动一格）做内积得到另外一个值-1，假设这个-1是另外一个neural的output，这个neural连接到input的（2,3,4，8,9,10,14，15,16），同样的weight代表同样的颜色。

在fully connect里面的两个neural本来是有自己的weight，当我们在做convolution时，首先把每一个neural连接的wight减少，强迫这两个neural共用一个weight。这件事就叫做shared weight，当我们做这件事情的时候，我们用的这个参数就比原来的更少。

![在这里插入图片描述](https://img-blog.csdnimg.cn/1476bd2ed8ab4a45aa2b4b871fc47651.png#pic_center =550x)
<center>▲ Convolution和Fully Connected之间的关系 </center>

## 五、最大池化（Max Pooling）
相对于convolution来说，Max Pooling是比较简单的。我们根据filter 1得到4*4的maxtrix，根据filter2得到另一个4 *4的matrix，接下来把output ，4个一组。每一组里面可以选择它们的平均（平均池化）或者选最大（最大池化）的都可以，就是把四个value合成一个value。这个可以让image缩小。

池化本质上的作用是：缩小图像，减少特征。

![在这里插入图片描述](https://img-blog.csdnimg.cn/c41db83d336844a59aee1a9d69fb8134.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/97c5ec93daf94c66850241e59c822552.png)
<p align="center">▲ Max Pooling </center>

## 六、压平（Flatten）
把多维的输入一维化，常用在从卷积层到全连接层的过渡。

flatten就是feature map拉直，拉直之后就可以丢到fully connected feedforward netwwork。

![在这里插入图片描述](https://img-blog.csdnimg.cn/b5fe136d2edd486c9ac33c162b109bb4.png)
<p align="center">▲ Flatten </center>

## 七、To learn more ......
- The methods of visualization in these slides
https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html
- More about visualization
 http://cs231n.github.io/understanding-cnn/
 - Very cool CNN visualization toolkit
 http://yosinski.com/deepvis
http://scs.ryerson.ca/~aharley/vis/conv/

 **How to let machine draw an image**
 - PixelRNN：https://arxiv.org/abs/1601.06759
- Variation Autoencoder (VAE)： https://arxiv.org/abs/1312.6114
 - Generative Adversarial Network (GAN)： http://arxiv.org/abs/1406.2661

## 八、总结
Datawhale组队学习，李宏毅《机器学习》Task6. Convolutional Neural Network（卷积神经网络）。包括为什么CNN用于图像处理、CNN架构、卷积（Convolution）、Convolution和Fully Connected之间的关系、最大池化（Max Pooling）、压平（Flatten）和其他的一些参考资料。

主要是原理部分的介绍，在实际运用中可能一行代码就实现了对应的功能。对于初学者而言，很多地方我觉得可以不求甚解，等到具体应用的时候再深挖一下，这样可以提高效率。
