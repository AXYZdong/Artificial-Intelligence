## 一、局部最小值与鞍点

Optimization Fails because .....

- local minima（局部最小值）
- saddle point（鞍点）

![在这里插入图片描述](https://img-blog.csdnimg.cn/181e028d46964708b2d9f16cae4fc133.png)
<p align="center">▲ 优化失败原因 </center>


## 二、批次与动量
- Optimization with Batch

![在这里插入图片描述](https://img-blog.csdnimg.cn/f394d81272ad43d4828bf64dcc7fbbef.png)
<p align="center">▲ 批次优化 </center>

- Gradient Descent + Momentum
![在这里插入图片描述](https://img-blog.csdnimg.cn/cefad43484a6491a82efa4b123199d56.png)
<p align="center">▲ 带动量的梯度下降 </center>


## 三、自动调整学习速率

**RMSProp(root mean square prop，均方根)**
学习原理∶在自适应梯度基础上引入了衰减因子，在梯度累积的时候，会对“过去”与“现在”做一个平衡，通过超参数进行调节衰减量。
适合处理非平稳目标（也就是与时间有关的)，对于RNN效果很好。

![在这里插入图片描述](https://img-blog.csdnimg.cn/720c0688c42c4244bc7b0d992cdc74d4.png)
<p align="center">▲ RMSProp </center>


**Adam(Adaptive momentum optimization，自适应动量优化)**
是目前深度学习中最流行的优化方法，它结合了自适应梯度善于处理稀疏梯度和均方根善于处理非平稳目标的优点，适用于大数据集和高维空间。
![在这里插入图片描述](https://img-blog.csdnimg.cn/bb1911522067451b8a9f4d88b02db7ed.png)
<p align="center">▲ Adam </center>

## 四、损失函数的影响
主要是对分类任务的影响。

![在这里插入图片描述](https://img-blog.csdnimg.cn/8e0a67edf95941d7907b478afd35a3d2.png)
<p align="center">▲ Loss of Classification </center>


## 五、批次标准化
批次标准化（Batch Normalization），改变不同特征的范围（changing landscape
）。

![在这里插入图片描述](https://img-blog.csdnimg.cn/e05e103a9d8a49a0a504bb429b98cc72.png)
<p align="center">▲ Changing Landscape </center>

![在这里插入图片描述](https://img-blog.csdnimg.cn/53f121b400dc4bfd8388482685a5e215.png)
<p align="center">▲ Feature Normalization </center>


## 五、总结
Datawhale组队学习，李宏毅《机器学习》Task5. Tips for neural network design（神经网络设计技巧）。包括局部最小值与鞍点、批次与动量、自动调整学习速率、损失函数的影响和批次标准化。李老师课程中对数学的原理讲得很清楚，不过对于侧重应用可以不求甚解，知道设计的技巧即可。
